# 广义线性回归
要求的$y$值是分类的

## 逻辑回归
二分类问题

### Generalized linear model
$f(x) = w^Tx + b \rightarrow y = w^Tx + b$
Log-linear regression$ln y = w^Tx + b$
$y = g^{-1}(w^Tx + b)$

sigmoid function 分成0、1
$z = w^Tx + b \in \mathbb{R} \rightarrow \{0, 1\}$
$y = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w^Tx + b)}}$

一般阈值0.5就好了，但是对于那些小概率的事件，我们需要调整阈值

### Logistic regression
$ln \frac{y}{1-y} = w^Tx + b$

即有

$ln \frac{P(y = 1|x)}{p(y = 0|x)} = w^Tx + b$

然后就可以解方程
$p(y=1|x) = p_1(x) = \frac{e^{w^Tx + b}}{1 + e^{w^Tx + b}}$
$p(y=0|x) = p_0(x) = \frac{1}{1 + e^{w^Tx + b}}$

### Maximum likelihood estimation
利用最大似然估计求解$w$和$b$
$P(D_C|\theta_c) = \Pi_{x_i \in D_C}P(x_i|\theta_c)$

都是乘法很容易小数一起乘没了

Log-likelihood $LL(\theta_c) = logP(D_C|\theta_p) = \sum_{x_i\in D_C}logP(x_i|\theta_c)$

The maximum likelihood eestimation of $\theta_c$ is $\hat{\theta_c} = arg\quad max_{\theta_c} LL(\theta_c)$

于是对于$w$和$b$，我们可以这样
$l(w,b)=ln(\Pi_{i=1}^m p_1(x_i|w,b)^{y_i}p_0(x_i|w,b)^{1-y_i}) \\ = \sum^m_{i=1}[y_ilnp1(x_i|w,b)+(1-y_i)lnp_0(x_i|w,b)]$

令$\beta = (w;b), \hat{x} = (x;1)$
简化算式$w^x+b = \beta^T\hat{x}$
$p_1(\hat{x_i})=\frac{e^{\beta^T\hat{x_i}}}{1+e^{\beta^T\hat{x_i}}}$
$p_0(\hat{x_i})=\frac{1}{1+e^{\beta^T\hat{x_i}}}$

就有了
$l(w,b)=l(\beta)=\sum^m_i[y_i\beta^T\hat{x_i} - ln(1+e^{\beta^T\hat{x_i}})]$

取负值，就可以求最小值
$\beta^{*} = arg \quad min_{\beta} -l(\beta)$


### 求解最小值
Newton's Method不仅利用了一阶导数，还利用了二阶导数，更加高效，沿着这个方向能更快到达驻点

发现步长很小很小的时候就到了极值点

对于$min f(x)$
泰勒展开around $x_0$
$f(x) = f(x_0 + \Delta x) \approx f(x_0) + f'(x_0)\Delta x + \frac{1}{2}f''(x_0)\Delta x^2 \triangleq g(x)$

我们找出$g(x)$的最小值即可
令$g'(x) = 0$
有$f'(x_0) + f''(x_0)\Delta x = 0$
$\Rightarrow \Delta x = - \frac{f'(x_0)}{f''(x_0)'}$这就是the Newton Direction

Steps of the Newton Method
- Given $x_0$, set $k := 0$
- $d^k = -\frac{f'(x_0)}{f''(x_0)'}$, if $\lVert d^k \rVert \le \varepsilon$, then stop
- Set $x^{k+1} \leftarrow x^k + d^k, kk \leftarrow k + 1$, go to step 1

### 实例分析
使用`glm`函数
```
fit1 = glm(formula = label ~ gender + age + yearsmarried + children + religiousness + education + occupation + rating, family = binomial(), data = Affairs)
summay(fit1)
```

输出结果
![](./img/glm.PNG)

可以看到对于gendermale/childrenyes/education/occupation的p值都大于0.05，因此我们去掉这些影响因素

于是就有了右边这个

### 拟合优度
非线性模型一般不存在平方和分解公式，于是Stata定义Pseudo $R^2 = \frac{ln L_0 - ln L_1}{ln L0}$
$L_1$为原模型的对数似然函数的最大值
$L_0$为**以常数项为唯一解释变量**的对数似然函数的最大值

由于$y$两点分布，似然函数的最大可能值为1，故对数似然函数的最大可能值为0，记为$ln L_{max} = 0$
也可以写成Pseudo $R^2 = \frac{ln L_0 - ln L_1}{ln L_{max} - ln L0}$


还有其他评价指标
- 准确率
- 灵敏度、召回率(查全率)
- 特异度
- 精准率
- F1值：$F1 = \frac{2Precision * Recall}{Precision + Recall}$
- 误报率
- 受试者工作特征曲线ROC(Receiver Operating Characteristic Curve)

#### ROC
ROC曲线以真阳性率（True Positive Rate，又称敏感度、灵敏度）为纵轴，以假阳性率（False Positive Rate）为横轴进行绘制。

真阳性率是指**被正确分类为正例的样本数与真实正例样本总数之比**，假阳性率是指**被错误分类为正例的负例样本数与真实负例样本总数之比**。
- TPR = 真的预测为真的个数/所有真的个数，真的里面多少比例被预测正确了，越大越好，**灵敏度**(Sensitivity)
- FPR = 假的预测为真的个数/所有假的个数，假的里面多少被误判为真了，越小越好，**误报率**(1-Specificity)

生成ROC曲线的过程是通过在分类器的输出阈值上进行变化，并计算对应的真阳性率和假阳性率，绘制出一系列的点，并以这些点为基础描绘出曲线。曲线上的每个点对应着不同的分类阈值，ROC曲线能够**展示出在不同阈值情况下分类器的综合性能**。

ROC曲线的形状可以用来评估分类器的性能。一般而言，**曲线越靠近左上角**，表示分类器具有**较高的真阳性率和较低的假阳性率**，即分类器的**性能较好**。**曲线下面积（Area Under the Curve，AUC）也是一个常用的评估指标，AUC值越接近1，表示分类器性能越佳。**

R代码
```R
par(mfrow=(1, 2))
library(ROCR)

plot_roc = function(fit) {
    p = predict(fit, Affairs, type = "response")
    pred = prediction(p, Affairs$label)
    (auc = performance(pred, "auc")@y.value)
    plot(performance(pred, "tpr", "fpr"), colorize = T, lwd = 3, main = "ROC curve")
    abline(a = 0, b = 1, lty = 2, lwd = 3, col = "black")
}

(plot_roc(fit1))
(plot_roc(fit2))
```

在ROC曲线（接收者操作特征曲线）上，找到最佳的截断点通常涉及权衡敏感性（True Positive Rate）和特异性（1 - False Positive Rate）之间的关系。最佳截断点是在这两个指标中找到一个平衡，以满足问题的具体需求。

以下是在R语言中找到最佳截断点的一般步骤：

1. **生成ROC曲线：** 使用R中的相关包（如`pROC`、`ROCR`等），计算出模型的ROC曲线。这通常涉及使用模型的预测概率或分数来计算不同截断点下的真正例率和假正例率。

   ```R
   # 以pROC包为例
   library(pROC)
   
   # 假设predictions是模型的预测概率，labels是实际标签（0或1）
   roc_curve <- roc(labels, predictions)
   ```

2. **选择最佳截断点：** 
   1. 在ROC曲线上，最佳截断点通常是使得**敏感性和特异性之差最小的点。这可以通过查找ROC曲线上最靠近左上角（0, 1点）的点来实现**。

    ```R
    # 找到最佳截断点
    best_threshold <- coords(roc_curve, "best", ret = "threshold")
    ```
    
   2. Youden Index是一种常用于选择最佳截断点的指标，它定义为敏感性（True Positive Rate）和特异性（1 - False Positive Rate）之差，即：

    \[ Youden Index = \text{Sensitivity} + \text{Specificity} - 1 \]

    最佳截断点是使得Youden Index最大化的截断点。在R中，可以使用以下代码找到最佳截断点：

    ```R
    # 计算Youden Index
    youden_index <- coords(roc_curve, "best", ret = "youden")

    # 找到最佳截断点
    best_threshold <- coords(roc_curve, "best", ret = "threshold", best.method = "youden")
    ```

   3. 最小损失（Cost Criterion）：

    最小损失方法涉及将分类问题的代价考虑在内，考虑到不同类型的错误可能会导致不同的损失。这通常涉及构建一个损失矩阵，其中包含了不同类型的分类错误所带来的损失。然后，通过在ROC曲线上选择最小化总损失的截断点来找到最佳截断点。

    在R中，可以使用以下代码通过最小损失方法找到最佳截断点：

    ```R
    # 定义损失矩阵
    loss_matrix <- matrix(c(0, 1, 5, 0), ncol = 2)

    # 计算最小损失截断点
    min_loss_threshold <- coords(roc_curve, "best", ret = "threshold", best.method = "closest.topleft", cost = loss_matrix)
    ```

1. **评估模型性能：** 使用最佳截断点评估模型的性能，计算混淆矩阵、准确率、精确度等指标。

   ```R
   # 根据最佳截断点创建混淆矩阵
   predictions_binary <- ifelse(predictions > best_threshold, 1, 0)
   confusion_matrix <- table(Actual = labels, Predicted = predictions_binary)
   ```

## 泊松回归
